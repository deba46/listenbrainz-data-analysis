{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/miniconda3/envs/music/bin/python\n",
      "Python version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]\n",
      "Current working directory: /Users/zodenath/Desktop/projects/listenbrainz-data-analysis/notebooks\n",
      "Platform: macOS-15.6-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "# Env check\n",
    "import sys, os, platform\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as psf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zodenath/Desktop/projects/listenbrainz-data-analysis/jars/postgresql-42.6.0.jar'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Jar path needed for spark session\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    proj_dir = os.path.abspath(\"..\")\n",
    "else:\n",
    "    proj_dir = cwd\n",
    "jar_dir = os.path.join(proj_dir, \"jars\")\n",
    "jar = os.path.join(jar_dir, \"postgresql-42.6.0.jar\")\n",
    "jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/08/10 21:43:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Listenbrainz process\") \\\n",
    "                    .config(\"spark.jars\", jar) \\\n",
    "                    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "                    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.json(f\"{proj_dir}/data/bronze/dataset.txt\")\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"listened_at\", LongType(), True),\n",
    "    StructField(\"recording_msid\", StringType(), True),\n",
    "    StructField(\"track_metadata\", StructType([\n",
    "        StructField(\"track_name\", StringType(), True),\n",
    "        StructField(\"artist_name\", StringType(), True),\n",
    "        StructField(\"release_name\", StringType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"user_name\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+---------+\n",
      "|listened_at|      recording_msid|      track_metadata|user_name|\n",
      "+-----------+--------------------+--------------------+---------+\n",
      "| 1555286560|1e1b2aa0-b2db-42e...|{Love In the Time...|  NichoBI|\n",
      "| 1555286378|283062c8-75e2-406...|{Cornflake, Withe...|  NichoBI|\n",
      "| 1555286137|8fe0c93d-f44a-415...|{Providence, With...|  NichoBI|\n",
      "| 1555246360|5998f0ac-2350-4ee...|{Boots of Spanish...|  NichoBI|\n",
      "| 1555246191|e3912d35-54e4-421...|{Trouble, Cat Ste...|  NichoBI|\n",
      "+-----------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Reading raw data from bronze storage layer as per required schema\n",
    "df = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .json(f\"{proj_dir}/data/bronze/dataset.txt\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_flat = df.select(\n",
    "    psf.col(\"user_name\"),\n",
    "    psf.from_unixtime(\"listened_at\").cast(\"timestamp\").alias(\"listened_at_ts\"),\n",
    "    psf.to_date(psf.from_unixtime(\"listened_at\")).alias(\"listened_date\"),\n",
    "    psf.col(\"recording_msid\"),\n",
    "    psf.col(\"track_metadata.track_name\").alias(\"track_name\"),\n",
    "    psf.col(\"track_metadata.artist_name\").alias(\"artist_name\"),\n",
    "    psf.col(\"track_metadata.release_name\").alias(\"release_name\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- listened_at_ts: timestamp (nullable = true)\n",
      " |-- listened_date: date (nullable = true)\n",
      " |-- recording_msid: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- release_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid recording IDs :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|recording_msid                      |\n",
      "+------------------------------------+\n",
      "|aba7cc6a-xxxx-4a27-88aa-c4c7c5fb5d83|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sample Data Validation : based on Assumption that recording ID is of particular format (E.g UUID )\n",
    "# \n",
    "uuid_regex = \"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$\"\n",
    "# Filter rows where UUID is invalid ; created a sample Invalid record\n",
    "df_invalid_uuid = df_flat.filter(~psf.col(\"recording_msid\").rlike(uuid_regex))\n",
    "print(\"Invalid recording IDs :\")\n",
    "df_invalid_uuid.select(\"recording_msid\").distinct().show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------+------------------------------------+------------------+--------------+---------------+\n",
      "|user_name     |listened_at_ts     |listened_date|recording_msid                      |track_name        |artist_name   |release_name   |\n",
      "+--------------+-------------------+-------------+------------------------------------+------------------+--------------+---------------+\n",
      "|6d6f7274686f6e|2019-02-12 09:29:59|2019-02-12   |000a98d0-02ac-4206-afde-5c985a5e4dcc|Hypersleep        |Volkor X      |This Means War |\n",
      "|6d6f7274686f6e|2019-03-01 20:52:41|2019-03-01   |009ed5a8-6bca-4c42-bbdc-66bd7c282c4b|Echoes of Divinity|George Kollias|INVICTUS       |\n",
      "|6d6f7274686f6e|2019-01-26 23:44:52|2019-01-26   |00b970e6-6b69-4667-8c5b-f0f555d7e372|The First Supper  |Daughters     |Daughters      |\n",
      "|6d6f7274686f6e|2019-01-29 09:30:21|2019-01-29   |00b970e6-6b69-4667-8c5b-f0f555d7e372|The First Supper  |Daughters     |Daughters      |\n",
      "|6d6f7274686f6e|2019-02-03 12:39:10|2019-02-03   |00b970e6-6b69-4667-8c5b-f0f555d7e372|The First Supper  |Daughters     |Daughters      |\n",
      "|6d6f7274686f6e|2019-03-18 07:55:31|2019-03-18   |00b970e6-6b69-4667-8c5b-f0f555d7e372|The First Supper  |Daughters     |Daughters      |\n",
      "|6d6f7274686f6e|2019-01-13 11:09:55|2019-01-13   |01745816-5b8a-433d-8e2e-6f7599f31091|Rebuild           |andromida     |More Than Human|\n",
      "|6d6f7274686f6e|2019-01-12 12:38:25|2019-01-12   |017c77b9-ecc5-4079-abe2-5be090aeb330|Jones From Indiana|Daughters     |Canada Songs   |\n",
      "|6d6f7274686f6e|2019-01-31 22:58:01|2019-01-31   |017c77b9-ecc5-4079-abe2-5be090aeb330|Jones From Indiana|Daughters     |Canada Songs   |\n",
      "|6d6f7274686f6e|2019-03-21 13:53:18|2019-03-21   |01b5386b-e61f-4ede-ab76-fa6202bb0554|Malina            |Leprous       |Malina         |\n",
      "+--------------+-------------------+-------------+------------------------------------+------------------+--------------+---------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_flat = df_flat.filter(psf.col(\"recording_msid\").rlike(uuid_regex))\n",
    "df_final = df_flat.dropDuplicates([\"user_name\", \"recording_msid\", \"listened_at_ts\"])\n",
    "df_final.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "333033"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to postgres database . staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"config/db_config.yaml\", \"r\") as f:\n",
    "    conf = yaml.safe_load(f)\n",
    "\n",
    "postgres_url = f\"jdbc:postgresql://{conf['host']}:{conf['port']}/{conf['database']}\"\n",
    "properties = {\n",
    "    \"user\": conf[\"user\"],\n",
    "    \"password\": conf[\"password\"],\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.write.jdbc(url=postgres_url, table=\"music.user_listens_staging\", mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering Bi quries in Pysaprk\n",
    "- Who are the top 10 users with respect to number of songs listened to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===========>                                              (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|     user_name|songs_listened|\n",
      "+--------------+--------------+\n",
      "|           hds|         46885|\n",
      "|       Groschi|         14959|\n",
      "| Silent Singer|         13005|\n",
      "|         phdnk|         12861|\n",
      "|6d6f7274686f6e|         11544|\n",
      "|      reverbel|          8398|\n",
      "|    Cl�psHydra|          8318|\n",
      "|InvincibleAsia|          7804|\n",
      "|      cimualte|          7356|\n",
      "|         inhji|          6349|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_10_users = (\n",
    "    df_final.groupBy(\"user_name\")\n",
    "           .agg(psf.count(\"*\").alias(\"songs_listened\"))\n",
    "           .orderBy(psf.desc(\"songs_listened\"))\n",
    "           .limit(10)\n",
    ")\n",
    "top_10_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================>                                  (4 + 6) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users on 2019-03-01: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q. How many users did listen to some song on the 1st of March 2019?\n",
    "users_march1 = (\n",
    "    df_final.filter(psf.col(\"listened_date\") == psf.lit(\"2019-03-01\"))\n",
    "           .select(\"user_name\")\n",
    "           .distinct()\n",
    "           .count()\n",
    ")\n",
    "print(f\"Number of users on 2019-03-01: {users_march1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=================>                                       (3 + 7) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------------+\n",
      "|      user_name|          track_name|     listened_at_ts|\n",
      "+---------------+--------------------+-------------------+\n",
      "| 6d6f7274686f6e|  The Leper Affinity|2019-01-01 11:41:51|\n",
      "|  Adsky_traktor|Сердце с долгом р...|2019-01-01 10:24:44|\n",
      "|      AllSparks|               Fever|2019-01-02 09:48:19|\n",
      "|   AlwinHummels|    Geef me je angst|2019-02-24 12:40:47|\n",
      "|          Arcor|     Exsultate Justi|2019-01-01 02:22:23|\n",
      "|AscendedGravity|              Amoeba|2019-01-02 00:01:17|\n",
      "|   Bezvezenator|              Devour|2019-01-01 07:19:22|\n",
      "|      BiamBioum|Beirut (14.12.16 ...|2019-01-07 14:56:07|\n",
      "|     BlackGauna|             Visionz|2019-01-01 19:36:09|\n",
      "|      Boris_Neo|      Keep You Close|2019-01-22 17:48:27|\n",
      "|   BornabeWylde|                Wow.|2019-04-14 16:01:05|\n",
      "|     Bound2Fate|       Home Invasion|2019-01-01 01:07:01|\n",
      "| Canis_L_Sapien|              Wolves|2019-01-03 09:39:29|\n",
      "|     Cl�psHydra|                 Hym|2019-01-01 14:09:35|\n",
      "|   Cooked_Bread|All These Things ...|2019-02-14 04:07:36|\n",
      "|     CraxorAdam|            Prophecy|2019-02-25 18:33:36|\n",
      "|       DJ.Xcite|              Top 20|2019-01-02 01:27:26|\n",
      "|         Dazzel|       The Departure|2019-01-03 11:26:56|\n",
      "|          Devvy|   Stop Torturing Me|2019-02-21 01:23:38|\n",
      "|DivineNightmare|              Birdie|2019-02-24 21:32:53|\n",
      "+---------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q. For every user, what was the first song the user listened to?\n",
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy(\"user_name\").orderBy(\"listened_at_ts\")\n",
    "\n",
    "first_song_per_user = (\n",
    "    df_final.withColumn(\"rn\", psf.row_number().over(w))\n",
    "           .filter(psf.col(\"rn\") == 1)\n",
    "           .select(\"user_name\", \"track_name\", \"listened_at_ts\")\n",
    ")\n",
    "first_song_per_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-------------+\n",
      "|     user_name|number_of_listens|listened_date|\n",
      "+--------------+-----------------+-------------+\n",
      "|6d6f7274686f6e|              200|   2019-01-27|\n",
      "|6d6f7274686f6e|              195|   2019-01-14|\n",
      "|6d6f7274686f6e|              193|   2019-01-16|\n",
      "| Adsky_traktor|              109|   2019-01-03|\n",
      "| Adsky_traktor|               99|   2019-01-05|\n",
      "| Adsky_traktor|               86|   2019-01-04|\n",
      "|     AllSparks|              114|   2019-01-31|\n",
      "|     AllSparks|               81|   2019-01-23|\n",
      "|     AllSparks|               71|   2019-01-07|\n",
      "|  AlwinHummels|                1|   2019-02-24|\n",
      "+--------------+-----------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Q. the top 3 days on which User had the most listens\n",
    "w_top3 = Window.partitionBy(\"user_name\").orderBy(psf.desc(\"number_of_listens\"))\n",
    "\n",
    "top3_days_per_user = (\n",
    "    df_final.groupBy(\"user_name\", \"listened_date\")\n",
    "           .agg(psf.count(\"*\").alias(\"number_of_listens\"))\n",
    "           .withColumn(\"rn\", psf.row_number().over(w_top3))\n",
    "           .filter(psf.col(\"rn\") <= 3)\n",
    "           .orderBy(\"user_name\", psf.desc(\"number_of_listens\"))\n",
    "           .select(\"user_name\", \"number_of_listens\", \"listened_date\")\n",
    ")\n",
    "top3_days_per_user.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:============================>                            (5 + 5) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------------------+\n",
      "|listened_date|number_active_users|percentage_active_users|\n",
      "+-------------+-------------------+-----------------------+\n",
      "|   2019-01-01|                 70|      34.65346534653465|\n",
      "|   2019-01-02|                 95|      47.02970297029702|\n",
      "|   2019-01-03|                102|     50.495049504950494|\n",
      "|   2019-01-04|                107|      52.97029702970298|\n",
      "|   2019-01-05|                108|      53.46534653465347|\n",
      "|   2019-01-06|                110|      54.45544554455446|\n",
      "|   2019-01-07|                114|      56.43564356435643|\n",
      "|   2019-01-08|                113|     55.940594059405946|\n",
      "|   2019-01-09|                115|     56.930693069306926|\n",
      "|   2019-01-10|                115|     56.930693069306926|\n",
      "|   2019-01-11|                114|      56.43564356435643|\n",
      "|   2019-01-12|                113|     55.940594059405946|\n",
      "|   2019-01-13|                114|      56.43564356435643|\n",
      "|   2019-01-14|                119|      58.91089108910891|\n",
      "|   2019-01-15|                117|     57.920792079207914|\n",
      "|   2019-01-16|                120|       59.4059405940594|\n",
      "|   2019-01-17|                120|       59.4059405940594|\n",
      "|   2019-01-18|                118|     58.415841584158414|\n",
      "|   2019-01-19|                119|      58.91089108910891|\n",
      "|   2019-01-20|                118|     58.415841584158414|\n",
      "+-------------+-------------------+-----------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#c) Daily active Users\n",
    "\n",
    "users_per_day = df_final.select(\"listened_date\", \"user_name\").distinct()\n",
    "\n",
    "# get distinct dates\n",
    "all_dates = users_per_day.select(\"listened_date\").distinct()\n",
    "\n",
    "# Join each date to users active in last 7 days\n",
    "active_users_window = (\n",
    "    all_dates.alias(\"d\")\n",
    "    .join(users_per_day.alias(\"u\"),\n",
    "          (psf.col(\"u.listened_date\") >= psf.date_sub(psf.col(\"d.listened_date\"), 6)) &\n",
    "          (psf.col(\"u.listened_date\") <= psf.col(\"d.listened_date\")),\n",
    "          \"left\")\n",
    "    .groupBy(\"d.listened_date\")\n",
    "    .agg(psf.countDistinct(\"u.user_name\").alias(\"number_active_users\"))\n",
    ")\n",
    "\n",
    "# Total unique users for percentage calc\n",
    "total_users = df_final.select(\"user_name\").distinct().count()\n",
    "\n",
    "# Add percentage column\n",
    "active_users_final = (\n",
    "    active_users_window\n",
    "    .withColumn(\"percentage_active_users\",\n",
    "                (psf.col(\"number_active_users\") / psf.lit(total_users) * 100))\n",
    "    .orderBy(\"listened_date\")\n",
    ")\n",
    "active_users_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
